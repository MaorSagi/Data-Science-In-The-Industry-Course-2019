{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/maorsagi/nlp-2019?scriptVersionId=109723952\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"!pip install git+https://github.com/goolig/dsClass.git","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:11:42.511699Z","iopub.execute_input":"2022-11-01T10:11:42.512205Z","iopub.status.idle":"2022-11-01T10:12:01.540335Z","shell.execute_reply.started":"2022-11-01T10:11:42.512154Z","shell.execute_reply":"2022-11-01T10:12:01.538703Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/goolig/dsClass.git\n  Cloning https://github.com/goolig/dsClass.git to /tmp/pip-req-build-r7caw1qp\n  Running command git clone -q https://github.com/goolig/dsClass.git /tmp/pip-req-build-r7caw1qp\nCollecting pydotplus\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/bf/62567830b700d9f6930e9ab6831d6ba256f7b0b730acb37278b0ccdffacf/pydotplus-2.0.2.tar.gz (278kB)\n\u001b[K     |████████████████████████████████| 286kB 908kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyparsing>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from pydotplus->dsClass==1.0.27) (2.4.5)\nBuilding wheels for collected packages: dsClass, pydotplus\n  Building wheel for dsClass (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for dsClass: filename=dsClass-1.0.27-cp36-none-any.whl size=16372735 sha256=8ac60fe511753f98636162e575abb65f5f66f1668a906955b2baa40260cc4f31\n  Stored in directory: /tmp/pip-ephem-wheel-cache-g0mzt4ht/wheels/c7/a9/03/023d8da4b4e004497715eeb1ea9b5d914ad425036bb4bc10b8\n  Building wheel for pydotplus (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pydotplus: filename=pydotplus-2.0.2-cp36-none-any.whl size=24566 sha256=3e661003c6358271ad93a4f1b806c605846438a6e7cfe9d60d32a046912f90b7\n  Stored in directory: /root/.cache/pip/wheels/35/7b/ab/66fb7b2ac1f6df87475b09dc48e707b6e0de80a6d8444e3628\nSuccessfully built dsClass pydotplus\nInstalling collected packages: pydotplus, dsClass\nSuccessfully installed dsClass-1.0.27 pydotplus-2.0.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\nimport nltk\n#nltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer as Lemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nimport pyLDAvis.gensim\nimport warnings\nwarnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarit\nfrom gensim.models import ldamulticore \nfrom gensim.corpora.dictionary import Dictionary\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport pickle\nfrom dsClass.path_helper import *\nprint('great it works')","metadata":{"_uuid":"145cdb642e1bfd46dfc3aa81656a2c51f89ab220","execution":{"iopub.status.busy":"2022-11-01T10:12:01.5452Z","iopub.execute_input":"2022-11-01T10:12:01.545929Z","iopub.status.idle":"2022-11-01T10:12:05.616965Z","shell.execute_reply.started":"2022-11-01T10:12:01.545568Z","shell.execute_reply":"2022-11-01T10:12:05.615946Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\ngreat it works\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip install pyLDAvis\n#!pip install gensim","metadata":{"_uuid":"1a02aa12d17fbb2de36e5c3cfa9d96b150d6d2d9","execution":{"iopub.status.busy":"2022-11-01T10:12:05.618625Z","iopub.execute_input":"2022-11-01T10:12:05.618987Z","iopub.status.idle":"2022-11-01T10:12:05.624313Z","shell.execute_reply.started":"2022-11-01T10:12:05.618925Z","shell.execute_reply":"2022-11-01T10:12:05.622319Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Feature selection ","metadata":{"_uuid":"4bde2c9669f3a6ec7239f8b527f71448cb445628"}},{"cell_type":"code","source":"# We create our corpus, we insert our data and split to tokens by newline char\n#text was taken from Wikipedia page - img \n#We don't have out features\n\n\ntext = '''The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n\nThe Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n\nSome notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n\nDuring the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n\nUp to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n\nMany of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n\nRecent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.\n\nIn recent years, there has been a flurry of results showing deep learning techniques[4][5] achieving state-of-the-art results in many natural language tasks, for example in language modeling,[6] parsing,[7][8] and many others.'''\n\ncorpus = [line for line in text.splitlines() if line != '']","metadata":{"_uuid":"3d2818a3cae1a6d93589c448bdf22e8fce87fe00","execution":{"iopub.status.busy":"2022-11-01T10:12:05.62599Z","iopub.execute_input":"2022-11-01T10:12:05.626279Z","iopub.status.idle":"2022-11-01T10:12:05.642739Z","shell.execute_reply.started":"2022-11-01T10:12:05.626221Z","shell.execute_reply":"2022-11-01T10:12:05.641311Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"corpus[0]\n","metadata":{"_uuid":"d04b5fe341f10cb8dc6ddbd0f3e214b5addb39c9","execution":{"iopub.status.busy":"2022-11-01T10:12:05.645836Z","iopub.execute_input":"2022-11-01T10:12:05.646135Z","iopub.status.idle":"2022-11-01T10:12:05.66217Z","shell.execute_reply.started":"2022-11-01T10:12:05.646076Z","shell.execute_reply":"2022-11-01T10:12:05.660596Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.'"},"metadata":{}}]},{"cell_type":"code","source":"len(corpus)","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:12:05.665441Z","iopub.execute_input":"2022-11-01T10:12:05.665803Z","iopub.status.idle":"2022-11-01T10:12:05.673807Z","shell.execute_reply.started":"2022-11-01T10:12:05.665731Z","shell.execute_reply":"2022-11-01T10:12:05.672149Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"8"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenization","metadata":{"_uuid":"b340881be3976dc993395ec015ea865818b62b18"}},{"cell_type":"code","source":"#We create table, every column is token, and each row is the sentence - our sample\n# worlds,world gives the same information, we want to decrease the number of features. high correlation\n \n\n\ncv = CountVectorizer(lowercase=False) #Create object of this class\ntermMatrix = cv.fit_transform(corpus) #fit- extract data and make set, tokenized by space char to tokens. After we did fit_transform it hold the features list.\n#here we insert all data, the model is learning, insert to data structure. \ndf = pd.DataFrame(data=termMatrix.toarray(),columns=cv.get_feature_names()) # 390 is all the features that i have (all tokens) in the text\ndf\n#each row gives me which token appears on each sentence (line on text, element in corpus array)","metadata":{"_uuid":"993aaf68951df169475f8f18f6b59d2b888c98ed","execution":{"iopub.status.busy":"2022-11-01T10:12:05.675804Z","iopub.execute_input":"2022-11-01T10:12:05.676196Z","iopub.status.idle":"2022-11-01T10:12:05.728456Z","shell.execute_reply.started":"2022-11-01T10:12:05.676122Z","shell.execute_reply":"2022-11-01T10:12:05.727509Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   1950  1950s  1954  1960s  1964  1966  1970s  1975  1976  1977  ...  \\\n0     1      1     0      0     0     0      0     0     0     0  ...   \n1     0      0     1      0     0     1      0     0     0     0  ...   \n2     0      0     0      1     1     1      0     0     0     0  ...   \n3     0      0     0      0     0     0      1     1     1     1  ...   \n4     0      0     0      0     0     0      0     0     0     0  ...   \n5     0      0     0      0     0     0      0     0     0     0  ...   \n6     0      0     0      0     0     0      0     0     0     0  ...   \n7     0      0     0      0     0     0      0     0     0     0  ...   \n\n   working  world  worlds  would  write  written  year  years  you  your  \n0        0      0       0      0      0        0     0      0    0     0  \n1        0      0       0      1      0        0     1      1    0     0  \n2        1      0       1      0      0        1     0      0    1     1  \n3        0      1       0      0      1        1     0      0    0     0  \n4        0      1       0      0      0        2     0      0    0     0  \n5        0      0       0      0      0        0     0      0    0     0  \n6        0      0       0      0      0        0     0      0    0     0  \n7        0      0       0      0      0        0     0      1    0     0  \n\n[8 rows x 390 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1950</th>\n      <th>1950s</th>\n      <th>1954</th>\n      <th>1960s</th>\n      <th>1964</th>\n      <th>1966</th>\n      <th>1970s</th>\n      <th>1975</th>\n      <th>1976</th>\n      <th>1977</th>\n      <th>...</th>\n      <th>working</th>\n      <th>world</th>\n      <th>worlds</th>\n      <th>would</th>\n      <th>write</th>\n      <th>written</th>\n      <th>year</th>\n      <th>years</th>\n      <th>you</th>\n      <th>your</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 390 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print (corpus[0])\ndf.iloc[0][df.iloc[0]>0] #first row,first sentence, print all features with value greater than 0","metadata":{"_uuid":"c108b64dfd811af315f9b154c783587f973a7c7c","execution":{"iopub.status.busy":"2022-11-01T10:12:05.72982Z","iopub.execute_input":"2022-11-01T10:12:05.73027Z","iopub.status.idle":"2022-11-01T10:12:05.743129Z","shell.execute_reply.started":"2022-11-01T10:12:05.730217Z","shell.execute_reply":"2022-11-01T10:12:05.742211Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"1950            1\n1950s           1\nAlan            1\nComputing       1\nIn              1\nIntelligence    1\nMachinery       1\nNLP             1\nThe             1\nTuring          2\nalthough        1\nan              1\nand             1\narticle         1\nas              1\nbe              1\ncalled          1\ncan             1\ncriterion       1\nearlier         1\nfound           1\nfrom            1\ngenerally       1\nhistory         1\nin              1\nintelligence    1\nis              1\nnow             1\nof              2\nperiods         1\nproposed        1\npublished       1\nstarted         1\ntest            1\nthe             2\ntitled          1\nwhat            1\nwhich           1\nwork            1\nName: 0, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Removing all punctuation, numbers and stop words","metadata":{"_uuid":"54ea0f8741fcb5a1bad50e46b39b34caef643a47"}},{"cell_type":"code","source":"cv = CountVectorizer(analyzer=\"word\",token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',stop_words=set(stopwords.words('english')))\n#analyzer=words , we want to cut , . ! etc.\n#token_pattern = we want just ABC letters\n#stop_words= cut stop words (as we discussed in the lecture)\ntermMatrix = cv.fit_transform(corpus)\ndf = pd.DataFrame(data=termMatrix.toarray(),columns=cv.get_feature_names())\ndf\n#The number of features become 301 instead of 390 -> ~25% less features","metadata":{"_uuid":"463ca6d4b8461d8886017ba46cc954442aa818dc","execution":{"iopub.status.busy":"2022-11-01T10:12:05.744582Z","iopub.execute_input":"2022-11-01T10:12:05.745093Z","iopub.status.idle":"2022-11-01T10:12:05.782305Z","shell.execute_reply.started":"2022-11-01T10:12:05.745026Z","shell.execute_reply":"2022-11-01T10:12:05.781184Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   able  accurate  achieving  advantage  alan  algorithms  almost  alpac  \\\n0     0         0          0          0     1           0       0      0   \n1     0         0          0          0     0           0       0      1   \n2     0         0          0          0     0           0       1      0   \n3     0         0          0          0     0           0       0      0   \n4     0         0          0          0     0           2       0      0   \n5     1         0          0          1     0           0       0      0   \n6     1         1          0          0     0           2       0      0   \n7     0         0          1          0     0           0       0      0   \n\n   although  among  ...  within  work  working  world  worlds  would  write  \\\n0         1      0  ...       0     1        0      0       0      0      0   \n1         0      0  ...       1     0        0      0       0      1      0   \n2         0      0  ...       0     0        1      0       1      0      0   \n3         0      0  ...       0     0        0      1       0      0      1   \n4         0      0  ...       0     0        0      1       0      0      0   \n5         0      0  ...       0     1        0      0       0      0      0   \n6         0      1  ...       0     0        0      1       0      0      0   \n7         0      0  ...       0     0        0      0       0      0      0   \n\n   written  year  years  \n0        0     0      0  \n1        0     1      1  \n2        1     0      0  \n3        1     0      0  \n4        2     0      0  \n5        0     0      0  \n6        0     0      0  \n7        0     0      1  \n\n[8 rows x 301 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>able</th>\n      <th>accurate</th>\n      <th>achieving</th>\n      <th>advantage</th>\n      <th>alan</th>\n      <th>algorithms</th>\n      <th>almost</th>\n      <th>alpac</th>\n      <th>although</th>\n      <th>among</th>\n      <th>...</th>\n      <th>within</th>\n      <th>work</th>\n      <th>working</th>\n      <th>world</th>\n      <th>worlds</th>\n      <th>would</th>\n      <th>write</th>\n      <th>written</th>\n      <th>year</th>\n      <th>years</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 301 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print (corpus[0])\ndf.iloc[0][df.iloc[0]>0]","metadata":{"_uuid":"f914d0b9aeff522766f3fd813aa5369437e406f3","execution":{"iopub.status.busy":"2022-11-01T10:12:05.784282Z","iopub.execute_input":"2022-11-01T10:12:05.78589Z","iopub.status.idle":"2022-11-01T10:12:05.80072Z","shell.execute_reply.started":"2022-11-01T10:12:05.784618Z","shell.execute_reply":"2022-11-01T10:12:05.798979Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"alan            1\nalthough        1\narticle         1\ncalled          1\ncomputing       1\ncriterion       1\nearlier         1\nfound           1\ngenerally       1\nhistory         1\nintelligence    2\nmachinery       1\nnlp             1\nperiods         1\nproposed        1\npublished       1\nstarted         1\ntest            1\ntitled          1\nturing          2\nwork            1\nName: 0, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Bigram","metadata":{"_uuid":"cae24f3a05d7dfa3e6f4f1415302c13cd337c2d0"}},{"cell_type":"code","source":"cv = CountVectorizer(analyzer=\"word\",token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',stop_words=set(stopwords.words('english')),ngram_range=(1,2))\n# Kind of Ngram - pairs of words, considiration of semantics, order between words.\n# N>3 is too much because it gives us 3 overlapping features and its going to make a lot of features.\n#4 words semantic don't gives us a lot, later in the evaluation we will see that its not significantly improve our model.\ntermMatrix = cv.fit_transform(corpus)\ndf = pd.DataFrame(data=termMatrix.toarray(),columns=cv.get_feature_names())\ndf\n","metadata":{"_uuid":"ee983ef16df9084c00d60b62d55bd58be02f06bf","execution":{"iopub.status.busy":"2022-11-01T10:12:05.802446Z","iopub.execute_input":"2022-11-01T10:12:05.803155Z","iopub.status.idle":"2022-11-01T10:12:05.836345Z","shell.execute_reply.started":"2022-11-01T10:12:05.802721Z","shell.execute_reply":"2022-11-01T10:12:05.835235Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   able  able learn  able take  accurate  accurate results  achieving  \\\n0     0           0          0         0                 0          0   \n1     0           0          0         0                 0          0   \n2     0           0          0         0                 0          0   \n3     0           0          0         0                 0          0   \n4     0           0          0         0                 0          0   \n5     1           0          1         0                 0          0   \n6     1           1          0         1                 1          0   \n7     0           0          0         0                 0          1   \n\n   achieving state  advantage  advantage existing  alan  ...  \\\n0                0          0                   0     1  ...   \n1                0          0                   0     0  ...   \n2                0          0                   0     0  ...   \n3                0          0                   0     0  ...   \n4                0          0                   0     0  ...   \n5                0          1                   1     0  ...   \n6                0          0                   0     0  ...   \n7                1          0                   0     0  ...   \n\n   write conceptual  written  written including  written joseph  \\\n0                 0        0                  0               0   \n1                 0        0                  0               0   \n2                 0        1                  0               1   \n3                 1        1                  1               0   \n4                 0        2                  0               0   \n5                 0        0                  0               0   \n6                 0        0                  0               0   \n7                 0        0                  0               0   \n\n   written rules  year  year long  years  years flurry  years machine  \n0              0     0          0      0             0              0  \n1              0     1          1      1             0              1  \n2              0     0          0      0             0              0  \n3              0     0          0      0             0              0  \n4              2     0          0      0             0              0  \n5              0     0          0      0             0              0  \n6              0     0          0      0             0              0  \n7              0     0          0      1             1              0  \n\n[8 rows x 710 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>able</th>\n      <th>able learn</th>\n      <th>able take</th>\n      <th>accurate</th>\n      <th>accurate results</th>\n      <th>achieving</th>\n      <th>achieving state</th>\n      <th>advantage</th>\n      <th>advantage existing</th>\n      <th>alan</th>\n      <th>...</th>\n      <th>write conceptual</th>\n      <th>written</th>\n      <th>written including</th>\n      <th>written joseph</th>\n      <th>written rules</th>\n      <th>year</th>\n      <th>year long</th>\n      <th>years</th>\n      <th>years flurry</th>\n      <th>years machine</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 710 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print (corpus[0])\ndf.iloc[0][df.iloc[0]>0]","metadata":{"_uuid":"d42539388610ff84c8c6d499f84d255e2172eac3","execution":{"iopub.status.busy":"2022-11-01T10:12:05.838242Z","iopub.execute_input":"2022-11-01T10:12:05.838585Z","iopub.status.idle":"2022-11-01T10:12:05.854182Z","shell.execute_reply.started":"2022-11-01T10:12:05.838513Z","shell.execute_reply":"2022-11-01T10:12:05.852446Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"alan                      1\nalan turing               1\nalthough                  1\nalthough work             1\narticle                   1\narticle titled            1\ncalled                    1\ncalled turing             1\ncomputing                 1\ncomputing machinery       1\ncriterion                 1\ncriterion intelligence    1\nearlier                   1\nearlier periods           1\nfound                     1\nfound earlier             1\ngenerally                 1\ngenerally started         1\nhistory                   1\nhistory nlp               1\nintelligence              2\nintelligence proposed     1\nmachinery                 1\nmachinery intelligence    1\nnlp                       1\nnlp generally             1\nperiods                   1\nperiods alan              1\nproposed                  1\nproposed called           1\npublished                 1\npublished article         1\nstarted                   1\nstarted although          1\ntest                      1\ntest criterion            1\ntitled                    1\ntitled computing          1\nturing                    2\nturing published          1\nturing test               1\nwork                      1\nwork found                1\nName: 0, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test set","metadata":{"_uuid":"9ad41db4e3b7d4b610ff34dbe96ddc1f2dbace94"}},{"cell_type":"markdown","source":"<img src=\"wiki_NLU.jpg\">","metadata":{"_uuid":"0ac5e9e2a5b03cebe9ebbf0c0745a2c159131976"}},{"cell_type":"code","source":"testText = '''The program STUDENT, written in 1964 by Daniel Bobrow for his PhD dissertation at MIT is one of the earliest known attempts at natural-language understanding by a computer.[6][7][8][9][10] Eight years after John McCarthy coined the term artificial intelligence, Bobrow's dissertation (titled Natural Language Input for a Computer Problem Solving System) showed how a computer could understand simple natural language input to solve algebra word problems.\n\nA year later, in 1965, Joseph Weizenbaum at MIT wrote ELIZA, an interactive program that carried on a dialogue in English on any topic, the most popular being psychotherapy. ELIZA worked by simple parsing and substitution of key words into canned phrases and Weizenbaum sidestepped the problem of giving the program a database of real-world knowledge or a rich lexicon. Yet ELIZA gained surprising popularity as a toy project and can be seen as a very early precursor to current commercial systems such as those used by Ask.com.[11]\n\nIn 1969 Roger Schank at Stanford University introduced the conceptual dependency theory for natural-language understanding.[12] This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.\n\nIn 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input.[13] Instead of phrase structure rules ATNs used an equivalent set of finite state automata that were called recursively. ATNs and their more general format called \"generalized ATNs\" continued to be used for a number of years.\n\nIn 1971 Terry Winograd finished writing SHRDLU for his PhD thesis at MIT. SHRDLU could understand simple English sentences in a restricted world of children's blocks to direct a robotic arm to move items. The successful demonstration of SHRDLU provided significant momentum for continued research in the field.[14][15] Winograd continued to be a major influence in the field with the publication of his book Language as a Cognitive Process.[16] At Stanford, Winograd would later be the adviser for Larry Page, who co-founded Google.\n\nIn the 1970s and 1980s the natural language processing group at SRI International continued research and development in the field. A number of commercial efforts based on the research were undertaken, e.g., in 1982 Gary Hendrix formed Symantec Corporation originally as a company for developing a natural language interface for database queries on personal computers. However, with the advent of mouse driven, graphic user interfaces Symantec changed direction. A number of other commercial efforts were started around the same time, e.g., Larry R. Harris at the Artificial Intelligence Corporation and Roger Schank and his students at Cognitive Systems corp.[17][18] In 1983, Michael Dyer developed the BORIS system at Yale which bore similarities to the work of Roger Schank and W. G. Lehnart.[19]\n\nThe third millennium saw the introduction of systems using machine learning for text classification, such as the IBM Watson. However, it is debated how much \"understanding\" such systems demonstrate, e.g. according to John Searle, Watson did not even understand the questions.[20]\n\nJohn Ball, cognitive scientist and inventor of Patom Theory supports this assessment. Natural language processing has made inroads for applications to support human productivity in service and ecommerce but this has largely been made possible by narrowing the scope of the application. There are thousands of ways to request something in a human language which still defies conventional natural language processing. \"To have a meaningful conversation with machines is only possible when we match every word to the correct meaning based on the meanings of the other words in the sentence – just like a 3-year-old does without guesswork\" Patom Theory'''\n\ntestCorpus = [line for line in testText.splitlines() if line != '']","metadata":{"_uuid":"c5c354698fa91c56a5f6fa7b7cf1f25b215d259c","execution":{"iopub.status.busy":"2022-11-01T10:12:05.85623Z","iopub.execute_input":"2022-11-01T10:12:05.856572Z","iopub.status.idle":"2022-11-01T10:12:05.865128Z","shell.execute_reply.started":"2022-11-01T10:12:05.856501Z","shell.execute_reply":"2022-11-01T10:12:05.863677Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"testTermMatrix = cv.transform(testCorpus)\ntestDf = pd.DataFrame(data=testTermMatrix.toarray(),columns=cv.get_feature_names())","metadata":{"_uuid":"fd763c66719f5e1d54d80c24bab7bc964003d2c3","execution":{"iopub.status.busy":"2022-11-01T10:12:05.866875Z","iopub.execute_input":"2022-11-01T10:12:05.867179Z","iopub.status.idle":"2022-11-01T10:12:05.884322Z","shell.execute_reply.started":"2022-11-01T10:12:05.867117Z","shell.execute_reply":"2022-11-01T10:12:05.883298Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print (testCorpus[0])\ntestDf.iloc[0][testDf.iloc[0]>0]","metadata":{"_uuid":"c5d618b07ac954b3882e9acac3a735fbd8261855","execution":{"iopub.status.busy":"2022-11-01T10:12:05.885999Z","iopub.execute_input":"2022-11-01T10:12:05.886349Z","iopub.status.idle":"2022-11-01T10:12:05.910551Z","shell.execute_reply.started":"2022-11-01T10:12:05.886289Z","shell.execute_reply":"2022-11-01T10:12:05.90939Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"The program STUDENT, written in 1964 by Daniel Bobrow for his PhD dissertation at MIT is one of the earliest known attempts at natural-language understanding by a computer.[6][7][8][9][10] Eight years after John McCarthy coined the term artificial intelligence, Bobrow's dissertation (titled Natural Language Input for a Computer Problem Solving System) showed how a computer could understand simple natural language input to solve algebra word problems.\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"computer            3\nearliest            1\ninput               2\nintelligence        1\nlanguage            3\nnatural             3\nnatural language    3\nproblem             1\nsystem              1\ntitled              1\nwritten             1\nyears               1\nName: 0, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Lemmatization","metadata":{"_uuid":"2991edebbbd6b5db6141f72ea2809a362805204a"}},{"cell_type":"code","source":"# transform to basic form.\n#lemmatizer - slower, have anlisys abilities\n#lives => live\nlemmatizer = Lemmatizer()\nprint (lemmatizer.lemmatize('ate',pos='v'))\n\nprint(lemmatizer.lemmatize('leaves'))","metadata":{"_uuid":"5109d0dfb9b3c343b00e55654ae3393976b84635","execution":{"iopub.status.busy":"2022-11-01T10:12:05.914082Z","iopub.execute_input":"2022-11-01T10:12:05.914426Z","iopub.status.idle":"2022-11-01T10:12:08.347787Z","shell.execute_reply.started":"2022-11-01T10:12:05.914363Z","shell.execute_reply":"2022-11-01T10:12:08.346181Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"eat\nleaf\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### As opposed to stemming","metadata":{"_uuid":"c08014bacaa2a8888c286761324eb185c10925a3"}},{"cell_type":"code","source":"#if he sees \"es\" or \"s\" it cuts it, very quickly, without analysis\nstemmer = PorterStemmer()\nprint (stemmer.stem('ate'))\nprint(stemmer.stem('leaves'))\n\n#we will alway choose one form! lemmatizer or stemmer not both","metadata":{"_uuid":"972fc47984ea94ee1a3f1d88ebbcad7683ee0109","execution":{"iopub.status.busy":"2022-11-01T10:12:08.349203Z","iopub.execute_input":"2022-11-01T10:12:08.349496Z","iopub.status.idle":"2022-11-01T10:12:08.356245Z","shell.execute_reply.started":"2022-11-01T10:12:08.349445Z","shell.execute_reply":"2022-11-01T10:12:08.355182Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"ate\nleav\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Topic modeling","metadata":{"_uuid":"ecb4d8ed1e5a900d90257991f15fdb908afb05d2"}},{"cell_type":"code","source":"# Load some categories \ncategories = [\n    'sci.space',\n    'alt.atheism',\n    'comp.graphics',\n    'rec.sport.baseball'\n]\n# For 4 categories we chose,we will sort to train & test.\n# We will work with data with tagged topic -> K topics. Here we choose K=4\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\ntrainDataset = fetch_20newsgroups(categories=categories,subset='train',shuffle=True, remove=('headers', 'footers', 'quotes'))\ntestDataset = fetch_20newsgroups(categories=categories,subset='test',shuffle=True, remove=('headers', 'footers', 'quotes'))\nprint(\"%d documents for training\" % len(trainDataset.data))\nprint(\"%d documents for testing\" % len(testDataset.data))\nprint(\"%d categories\" % len(trainDataset.target_names))","metadata":{"_uuid":"e49c4e209517a9d23a83ab6d9715ab951c19c316","execution":{"iopub.status.busy":"2022-11-01T10:12:08.357736Z","iopub.execute_input":"2022-11-01T10:12:08.358239Z","iopub.status.idle":"2022-11-01T10:12:23.716959Z","shell.execute_reply.started":"2022-11-01T10:12:08.358176Z","shell.execute_reply":"2022-11-01T10:12:23.715995Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Downloading 20news dataset. This may take a few minutes.\nDownloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n","output_type":"stream"},{"name":"stdout","text":"Loading 20 newsgroups dataset for categories:\n['sci.space', 'alt.atheism', 'comp.graphics', 'rec.sport.baseball']\n2254 documents for training\n1499 documents for testing\n4 categories\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data preparation ","metadata":{"_uuid":"f1c7138271bec80932362ae37ffc2c38b50a2ad2"}},{"cell_type":"markdown","source":"## Filter out small docs","metadata":{"_uuid":"a1f0f0a3915dca4fbc691815f22431f7ebf69b98"}},{"cell_type":"code","source":"\n\ndef filterSmallDocs(docs,targets):\n    indices = [i for i in range(0,len(docs)) if len(docs[i].split())>20]\n    filteredDocs = [docs[i] for i in indices]\n    filteredTarget = [targets[i] for i in indices]\n    return filteredDocs,filteredTarget","metadata":{"_uuid":"d673be5af5b9346153a4e71bd7f8ca68f44ed8d7","execution":{"iopub.status.busy":"2022-11-01T10:12:23.718962Z","iopub.execute_input":"2022-11-01T10:12:23.71934Z","iopub.status.idle":"2022-11-01T10:12:23.726876Z","shell.execute_reply.started":"2022-11-01T10:12:23.719279Z","shell.execute_reply":"2022-11-01T10:12:23.725524Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"trainDocs, trainTarget = filterSmallDocs(trainDataset.data,trainDataset.target)\ntestDocs, testTarget = filterSmallDocs(testDataset.data,testDataset.target)","metadata":{"_uuid":"9722c2a6f119125d5b8c84871dd050476ac97fc5","execution":{"iopub.status.busy":"2022-11-01T10:12:23.728872Z","iopub.execute_input":"2022-11-01T10:12:23.729449Z","iopub.status.idle":"2022-11-01T10:12:23.978886Z","shell.execute_reply.started":"2022-11-01T10:12:23.72922Z","shell.execute_reply":"2022-11-01T10:12:23.978036Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## How does the docs distribute over the topics","metadata":{"_uuid":"bba55509e19e55a2bf383f40ba27f476eb9aae9d"}},{"cell_type":"code","source":"pd.Series(trainTarget).value_counts().plot(kind=\"bar\")\npd.Series(testTarget).value_counts()","metadata":{"_uuid":"425770e4143a9eae3285889b3bff65412689cd8c","execution":{"iopub.status.busy":"2022-11-01T10:12:23.981043Z","iopub.execute_input":"2022-11-01T10:12:23.981404Z","iopub.status.idle":"2022-11-01T10:12:24.314921Z","shell.execute_reply.started":"2022-11-01T10:12:23.981347Z","shell.execute_reply":"2022-11-01T10:12:24.313439Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"3    349\n1    346\n2    338\n0    278\ndtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"pd.Series(testTarget).value_counts().plot(kind=\"bar\")\npd.Series(testTarget).value_counts()","metadata":{"_uuid":"c1f14615292d90e7c08bf0a79aa7e7283bcfb0c4","execution":{"iopub.status.busy":"2022-11-01T10:12:24.317231Z","iopub.execute_input":"2022-11-01T10:12:24.318051Z","iopub.status.idle":"2022-11-01T10:12:24.649371Z","shell.execute_reply.started":"2022-11-01T10:12:24.317968Z","shell.execute_reply":"2022-11-01T10:12:24.64795Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"3    349\n1    346\n2    338\n0    278\ndtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"cv = CountVectorizer(analyzer=\"word\",token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',stop_words='english',min_df=50, max_df=0.8, ngram_range=(1,2))\n#min_df = how many instances of the word in different docs- apper in more than 50 docs\n# max_df = if appears in more than 80% of docs its not relevant\ndata = cv.fit_transform(trainDocs)\ndata = pd.DataFrame(data.toarray(), columns=cv.get_feature_names())\ndata.iloc[0:3]","metadata":{"_uuid":"9943e3ecc795f61703508f7d6a69852b09d48457","execution":{"iopub.status.busy":"2022-11-01T10:12:24.651743Z","iopub.execute_input":"2022-11-01T10:12:24.652528Z","iopub.status.idle":"2022-11-01T10:12:25.992761Z","shell.execute_reply.started":"2022-11-01T10:12:24.652442Z","shell.execute_reply":"2022-11-01T10:12:25.992027Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"   able  access  actually  add  address  advance  ago  agree  al  answer  ...  \\\n0     0       0         0    0        0        0    0      0   0       0  ...   \n1     0       0         0    0        0        0    0      0   0       0  ...   \n2     0       0         0    0        0        0    0      0   0       0  ...   \n\n   working  works  world  wouldn  write  written  wrong  year  years  yes  \n0        0      0      0       0      0        0      0     0      0    0  \n1        0      0      0       0      0        0      0     0      0    0  \n2        0      0      0       0      0        0      2     0      0    0  \n\n[3 rows x 340 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>able</th>\n      <th>access</th>\n      <th>actually</th>\n      <th>add</th>\n      <th>address</th>\n      <th>advance</th>\n      <th>ago</th>\n      <th>agree</th>\n      <th>al</th>\n      <th>answer</th>\n      <th>...</th>\n      <th>working</th>\n      <th>works</th>\n      <th>world</th>\n      <th>wouldn</th>\n      <th>write</th>\n      <th>written</th>\n      <th>wrong</th>\n      <th>year</th>\n      <th>years</th>\n      <th>yes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 340 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Convert the term matrix into term lists","metadata":{"_uuid":"3bcdfa671030469e8b7b22a4540401bdb45934de"}},{"cell_type":"code","source":"#few manipulations of the data -> for making topic models\n#each row is doc after filtering\ncols = data.columns\nbt = data.apply(lambda x: x > 0)\nbt = bt.apply(lambda x: list(cols[x.values]), axis=1) #instead of numbers we want to see the words\nbt[0:3]","metadata":{"_uuid":"38614ab742da8c8a6754e1a32a9cd3fcd5c9e6e8","execution":{"iopub.status.busy":"2022-11-01T10:12:25.993911Z","iopub.execute_input":"2022-11-01T10:12:25.994299Z","iopub.status.idle":"2022-11-01T10:12:26.269132Z","shell.execute_reply.started":"2022-11-01T10:12:25.994257Z","shell.execute_reply":"2022-11-01T10:12:26.267725Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"0                 [built, don, don think, just, think]\n1    [ask, bit, change, don, file, heard, just, kno...\n2    [based, didn, edu, know, line, mac, programs, ...\ndtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"## Turn our tokenized documents into an id term, frequancy list","metadata":{"_uuid":"700f29296e1cc1b53c20343004bae646eb263a55"}},{"cell_type":"code","source":"# we have to make propper input for using some library.\n#first word - built, (0,1) - 0 is the ID and 1 is the number of instances\n\ndictionary = Dictionary(bt)\n# convert tokenized documents into a document-term matrix\ncorpusTopicModeling = [dictionary.doc2bow(text) for text in bt]\ncorpusTopicModeling[0:3]","metadata":{"_uuid":"3fae880035f3a9c99f1e09fbd69b1a02990b5913","execution":{"iopub.status.busy":"2022-11-01T10:12:26.27124Z","iopub.execute_input":"2022-11-01T10:12:26.271879Z","iopub.status.idle":"2022-11-01T10:12:26.391254Z","shell.execute_reply.started":"2022-11-01T10:12:26.271611Z","shell.execute_reply":"2022-11-01T10:12:26.390375Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n [(1, 1),\n  (3, 1),\n  (5, 1),\n  (6, 1),\n  (7, 1),\n  (8, 1),\n  (9, 1),\n  (10, 1),\n  (11, 1),\n  (12, 1),\n  (13, 1),\n  (14, 1),\n  (15, 1),\n  (16, 1),\n  (17, 1)],\n [(10, 1),\n  (18, 1),\n  (19, 1),\n  (20, 1),\n  (21, 1),\n  (22, 1),\n  (23, 1),\n  (24, 1),\n  (25, 1),\n  (26, 1),\n  (27, 1)]]"},"metadata":{}}]},{"cell_type":"code","source":"dictionary[0]","metadata":{"_uuid":"5c59111f581ef90908a7c1b96d8dd7c4924171a5","execution":{"iopub.status.busy":"2022-11-01T10:12:26.393399Z","iopub.execute_input":"2022-11-01T10:12:26.393998Z","iopub.status.idle":"2022-11-01T10:12:26.402517Z","shell.execute_reply.started":"2022-11-01T10:12:26.393775Z","shell.execute_reply":"2022-11-01T10:12:26.400831Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'built'"},"metadata":{}}]},{"cell_type":"code","source":"print(\"The model has %d features\" % (len(dictionary)))","metadata":{"_uuid":"5928a3b8af58574e2b6b38bbef3ab15fc070454c","execution":{"iopub.status.busy":"2022-11-01T10:12:26.404471Z","iopub.execute_input":"2022-11-01T10:12:26.404891Z","iopub.status.idle":"2022-11-01T10:12:26.415268Z","shell.execute_reply.started":"2022-11-01T10:12:26.404818Z","shell.execute_reply":"2022-11-01T10:12:26.414092Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"The model has 340 features\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Modeling","metadata":{"_uuid":"7543114b939a8bfa64085b3e04179653c8268ed0"}},{"cell_type":"code","source":"#takes time, 150 going to take a lot of time (10-20 minutes)\n#test with less than 150 first\n\n#ldaModel = ldamulticore.LdaMulticore(corpus=corpus, id2word=dictionary, passes=150, num_topics=4)","metadata":{"_uuid":"bddf762fea296ca51956166a2a205c92ffc5ee7a","execution":{"iopub.status.busy":"2022-11-01T10:12:26.416988Z","iopub.execute_input":"2022-11-01T10:12:26.417273Z","iopub.status.idle":"2022-11-01T10:12:26.426599Z","shell.execute_reply.started":"2022-11-01T10:12:26.417223Z","shell.execute_reply":"2022-11-01T10:12:26.425287Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\n#pickle - compact way to save the data\n\n#with open('ldaModel.pickle', 'wb') as handle:\n#    pickle.dump(ldaModel, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nwith open(get_file_path('ldaModel.pickle'), 'rb') as handle:\n    ldaModel = pickle.load(handle)","metadata":{"_uuid":"86a4dff167608a058ca35f6dea072d2f8782eb43","execution":{"iopub.status.busy":"2022-11-01T10:12:26.428121Z","iopub.execute_input":"2022-11-01T10:12:26.428406Z","iopub.status.idle":"2022-11-01T10:12:26.44111Z","shell.execute_reply.started":"2022-11-01T10:12:26.428361Z","shell.execute_reply":"2022-11-01T10:12:26.439533Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"pyLDAvis.enable_notebook()\n# 4 topics (catagories), we use tsne\n#red - show us frequency , shows us uniqueness (comparing to the blue part).\npyLDAvis.gensim.prepare(ldaModel, corpusTopicModeling,dictionary,R=20,mds=\"tsne\")","metadata":{"_uuid":"bd8df0ef183bdf58a10223e71806293ad2bc68d4","execution":{"iopub.status.busy":"2022-11-01T10:12:26.443335Z","iopub.execute_input":"2022-11-01T10:12:26.443848Z","iopub.status.idle":"2022-11-01T10:12:42.266641Z","shell.execute_reply.started":"2022-11-01T10:12:26.443743Z","shell.execute_reply":"2022-11-01T10:12:42.26546Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"PreparedData(topic_coordinates=               x           y  topics  cluster       Freq\ntopic                                                   \n1       7.335405 -277.268372       1        1  37.313919\n3     -84.281311 -232.048615       2        1  21.360069\n2     -39.064224 -140.430588       3        1  21.046087\n0      52.552490 -185.650314       4        1  20.279921, topic_info=    Category        Freq      Term       Total  loglift  logprob\n48   Default  235.000000     space  235.000000  20.0000  20.0000\n113  Default  199.000000    thanks  199.000000  19.0000  19.0000\n251  Default  236.000000      year  236.000000  18.0000  18.0000\n181  Default  129.000000  graphics  129.000000  17.0000  17.0000\n196  Default  120.000000      game  120.000000  16.0000  16.0000\n..       ...         ...       ...         ...      ...      ...\n11    Topic4  110.454834      like  469.698700   0.1481  -4.1189\n3     Topic4  102.687080      just  433.316803   0.1558  -4.1918\n74    Topic4   93.662056      time  331.047974   0.3330  -4.2838\n1     Topic4   84.001091       don  445.753723  -0.0734  -4.3926\n41    Topic4   74.475250        ll  183.090454   0.6960  -4.5130\n\n[153 rows x 6 columns], token_table=      Topic      Freq     Term\nterm                          \n165       2  0.988275  address\n239       2  0.969745  advance\n239       4  0.014919  advance\n325       1  0.976680    agree\n325       4  0.013198    agree\n...     ...       ...      ...\n251       3  0.265949     year\n251       4  0.730305     year\n51        1  0.128071    years\n51        3  0.430320    years\n51        4  0.435443    years\n\n[257 rows x 3 columns], R=20, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 4, 3, 1])","text/html":"\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n\n\n<div id=\"ldavis_el141400058176215048693157103\"></div>\n<script type=\"text/javascript\">\n\nvar ldavis_el141400058176215048693157103_data = {\"mdsDat\": {\"x\": [7.335404872894287, -84.28131103515625, -39.06422424316406, 52.552490234375], \"y\": [-277.26837158203125, -232.04861450195312, -140.4305877685547, -185.6503143310547], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [37.31391906738281, 21.360069274902344, 21.04608726501465, 20.27992057800293]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"Freq\": [235.0, 199.0, 236.0, 129.0, 120.0, 119.0, 118.0, 126.0, 113.0, 96.0, 162.0, 98.0, 128.0, 91.0, 88.0, 91.0, 87.0, 88.0, 409.0, 83.0, 111.4024887084961, 65.17278289794922, 70.0001220703125, 62.20193099975586, 53.35994338989258, 52.3683967590332, 51.3924446105957, 50.394805908203125, 74.22991943359375, 66.4133071899414, 51.80248260498047, 75.26951599121094, 44.03742599487305, 82.39234924316406, 101.8885726928711, 100.68598937988281, 64.95922088623047, 75.76264190673828, 186.27114868164062, 62.91318130493164, 96.60885620117188, 225.8057403564453, 127.98603057861328, 300.7615051269531, 260.06201171875, 124.7193603515625, 119.17150115966797, 229.3419189453125, 88.1075210571289, 123.87206268310547, 118.84351348876953, 161.10366821289062, 193.7386474609375, 136.6126251220703, 170.4699249267578, 127.9688491821289, 98.83369445800781, 113.15158081054688, 125.9278564453125, 110.44042205810547, 128.3516082763672, 117.4875259399414, 97.79217529296875, 90.90309143066406, 83.01822662353516, 90.81998443603516, 76.11325073242188, 75.11314392089844, 69.22377014160156, 70.1903305053711, 65.26929473876953, 55.42717361450195, 55.416507720947266, 53.45042419433594, 51.487552642822266, 49.50322723388672, 51.2933349609375, 194.37039184570312, 77.8723373413086, 65.15312957763672, 117.61163330078125, 74.32417297363281, 81.06792449951172, 113.57866668701172, 91.5360107421875, 87.38417053222656, 105.05376434326172, 98.9483413696289, 147.3605194091797, 115.6120376586914, 98.80025482177734, 93.80400085449219, 100.72945404052734, 234.90948486328125, 118.60901641845703, 87.51837921142578, 80.49479675292969, 72.46580505371094, 64.44364166259766, 63.42169952392578, 56.37336730957031, 54.391761779785156, 51.4017448425293, 75.93851470947266, 54.4496955871582, 104.8795166015625, 71.28019714355469, 75.31593322753906, 65.68153381347656, 48.267494201660156, 70.0819320678711, 46.00849533081055, 43.23806381225586, 53.62900161743164, 69.0287094116211, 70.63170623779297, 77.4559097290039, 89.1054916381836, 84.3756103515625, 81.29431915283203, 77.39006805419922, 82.15660858154297, 76.03178405761719, 119.33197784423828, 95.51786804199219, 87.57823944091797, 86.58032989501953, 76.66720581054688, 76.63744354248047, 72.69583129882812, 70.7093505859375, 67.71470642089844, 59.78315734863281, 55.826663970947266, 50.848602294921875, 49.85100555419922, 59.11093521118164, 74.78131103515625, 107.97665405273438, 77.5899658203125, 47.50935745239258, 37.798065185546875, 172.9053497314453, 79.08788299560547, 109.636962890625, 85.34982299804688, 110.07275390625, 84.59165954589844, 110.454833984375, 102.68708038330078, 93.66205596923828, 84.00109100341797, 74.47525024414062], \"Term\": [\"space\", \"thanks\", \"year\", \"graphics\", \"game\", \"nasa\", \"mail\", \"information\", \"earth\", \"games\", \"program\", \"image\", \"team\", \"files\", \"baseball\", \"file\", \"hit\", \"orbit\", \"think\", \"hi\", \"god\", \"religion\", \"simply\", \"claim\", \"atheism\", \"cause\", \"evidence\", \"exist\", \"agree\", \"argument\", \"words\", \"saying\", \"ones\", \"reason\", \"case\", \"mean\", \"means\", \"life\", \"say\", \"understand\", \"isn\", \"people\", \"thing\", \"don\", \"think\", \"said\", \"things\", \"just\", \"true\", \"really\", \"point\", \"does\", \"like\", \"way\", \"know\", \"make\", \"course\", \"ve\", \"time\", \"good\", \"graphics\", \"mail\", \"image\", \"files\", \"hi\", \"file\", \"email\", \"address\", \"ftp\", \"images\", \"info\", \"format\", \"color\", \"gif\", \"windows\", \"appreciated\", \"mac\", \"thanks\", \"code\", \"advance\", \"information\", \"computer\", \"software\", \"program\", \"looking\", \"available\", \"edu\", \"help\", \"know\", \"does\", \"use\", \"need\", \"like\", \"space\", \"nasa\", \"orbit\", \"launch\", \"moon\", \"shuttle\", \"station\", \"development\", \"built\", \"flight\", \"cost\", \"technology\", \"earth\", \"money\", \"research\", \"low\", \"date\", \"science\", \"near\", \"control\", \"national\", \"large\", \"data\", \"high\", \"new\", \"years\", \"long\", \"use\", \"time\", \"just\", \"game\", \"games\", \"baseball\", \"hit\", \"season\", \"win\", \"league\", \"players\", \"home\", \"teams\", \"pitching\", \"player\", \"ball\", \"average\", \"play\", \"team\", \"runs\", \"lost\", \"defense\", \"year\", \"lot\", \"good\", \"years\", \"think\", \"better\", \"like\", \"just\", \"time\", \"don\", \"ll\"], \"Total\": [235.0, 199.0, 236.0, 129.0, 120.0, 119.0, 118.0, 126.0, 113.0, 96.0, 162.0, 98.0, 128.0, 91.0, 88.0, 91.0, 87.0, 88.0, 409.0, 83.0, 112.16057586669922, 65.92196655273438, 70.84245300292969, 62.970741271972656, 54.11639404296875, 53.13272476196289, 52.14888381958008, 51.165184020996094, 75.76691436767578, 71.8659896850586, 57.08279800415039, 84.7313232421875, 50.228939056396484, 94.5522689819336, 118.34019470214844, 118.2412338256836, 76.95905303955078, 89.78932189941406, 225.64016723632812, 76.7870864868164, 118.28782653808594, 283.2070007324219, 164.70361328125, 445.75372314453125, 409.0042724609375, 167.8212890625, 163.03900146484375, 433.3168029785156, 111.447265625, 192.5714111328125, 187.26722717285156, 314.45648193359375, 469.6986999511719, 249.0255889892578, 403.5744934082031, 239.5172119140625, 143.29608154296875, 219.09243774414062, 331.0479736328125, 301.50421142578125, 129.10682678222656, 118.26648712158203, 98.55628967285156, 91.65767669677734, 83.77360534667969, 91.65760040283203, 76.87516021728516, 75.8897933959961, 69.97660827636719, 70.96231842041016, 66.03468322753906, 56.179527282714844, 56.179595947265625, 54.20863342285156, 52.237491607666016, 50.2665901184082, 52.240718841552734, 199.1056365966797, 79.82964324951172, 67.02793884277344, 126.13772583007812, 84.74283599853516, 97.85301208496094, 162.47654724121094, 126.30573272705078, 118.80641174316406, 162.78314208984375, 151.1642303466797, 403.5744934082031, 314.45648193359375, 246.7571563720703, 218.4195556640625, 469.6986999511719, 235.73330688476562, 119.36591339111328, 88.26756286621094, 81.24528503417969, 73.21984100341797, 65.1944351196289, 64.19098663330078, 57.16844940185547, 55.16233825683594, 52.15312194824219, 80.20423889160156, 58.11983108520508, 113.19791412353516, 78.16950988769531, 83.12506866455078, 78.11007690429688, 58.01072692871094, 87.9307861328125, 60.034934997558594, 56.91449737548828, 72.0207748413086, 99.7630386352539, 106.69464874267578, 124.74867248535156, 203.7927703857422, 195.20355224609375, 182.91677856445312, 246.7571563720703, 331.0479736328125, 433.3168029785156, 120.0916748046875, 96.2716064453125, 88.33155059814453, 87.33909606933594, 77.4140625, 77.4140396118164, 73.44404602050781, 71.45903778076172, 68.48149871826172, 60.54155731201172, 56.57146453857422, 51.608924865722656, 50.61641311645508, 61.55166244506836, 79.36530303955078, 128.23927307128906, 94.17443084716797, 61.422340393066406, 50.74595642089844, 236.88717651367188, 146.61598205566406, 301.50421142578125, 195.20355224609375, 409.0042724609375, 200.87045288085938, 469.6986999511719, 433.3168029785156, 331.0479736328125, 445.75372314453125, 183.0904541015625], \"loglift\": [20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9789999723434448, 0.974399983882904, 0.973800003528595, 0.9735000133514404, 0.9717000126838684, 0.9713000059127808, 0.9711999893188477, 0.9706000089645386, 0.9653000235557556, 0.9068999886512756, 0.888700008392334, 0.8673999905586243, 0.8543000221252441, 0.8481000065803528, 0.8360999822616577, 0.8251000046730042, 0.8162999749183655, 0.8159000277519226, 0.7940999865531921, 0.7864999771118164, 0.7833999991416931, 0.7592999935150146, 0.7336000204086304, 0.5924000144004822, 0.5329999923706055, 0.6890000104904175, 0.6723999977111816, 0.34950000047683716, 0.7508000135421753, 0.5446000099182129, 0.5310999751091003, 0.31700000166893005, 0.10019999742507935, 0.3853999972343445, 0.12399999797344208, 0.35899999737739563, 0.614300012588501, 0.32499998807907104, 0.019200000911951065, -0.01850000023841858, 1.5377999544143677, 1.5369999408721924, 1.5358999967575073, 1.5354000329971313, 1.534600019454956, 1.534500002861023, 1.5336999893188477, 1.533400058746338, 1.5327999591827393, 1.5326999425888062, 1.531999945640564, 1.5302000045776367, 1.5299999713897705, 1.5296000242233276, 1.5291999578475952, 1.5283000469207764, 1.5253000259399414, 1.5196000337600708, 1.5188000202178955, 1.5153000354766846, 1.4737000465393066, 1.412500023841858, 1.3554999828338623, 1.1856000423431396, 1.2216999530792236, 1.2365000247955322, 1.1057000160217285, 1.1198999881744385, 0.5361999869346619, 0.5429999828338623, 0.6283000111579895, 0.6984000205993652, 0.004000000189989805, 1.5549999475479126, 1.5520999431610107, 1.5499000549316406, 1.5492000579833984, 1.5480999946594238, 1.5469000339508057, 1.5463999509811401, 1.5444999933242798, 1.5443999767303467, 1.5439000129699707, 1.5038000345230103, 1.4931999444961548, 1.482100009918213, 1.4661999940872192, 1.4598000049591064, 1.385200023651123, 1.3746000528335571, 1.3315999507904053, 1.2924000024795532, 1.2835999727249146, 1.2635999917984009, 1.1901999711990356, 1.1460000276565552, 1.0819000005722046, 0.7311999797821045, 0.7196999788284302, 0.7475000023841858, 0.39890000224113464, 0.1648000031709671, -0.1818999946117401, 1.5892000198364258, 1.5877000093460083, 1.5870000123977661, 1.5867999792099, 1.585800051689148, 1.5855000019073486, 1.5852999687194824, 1.5850000381469727, 1.5843000411987305, 1.582900047302246, 1.5822999477386475, 1.5807000398635864, 1.580299973487854, 1.5550999641418457, 1.5360000133514404, 1.4235999584197998, 1.4018000364303589, 1.3387000560760498, 1.3009999990463257, 1.2806999683380127, 0.9782999753952026, 0.583899974822998, 0.7682999968528748, 0.28299999237060547, 0.7307000160217285, 0.14810000360012054, 0.155799999833107, 0.3330000042915344, -0.07339999824762344, 0.6959999799728394], \"logprob\": [20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.71999979019165, -5.256199836730957, -5.184700012207031, -5.302800178527832, -5.456099987030029, -5.474899768829346, -5.49370002746582, -5.513299942016602, -5.125999927520752, -5.237299919128418, -5.485799789428711, -5.112100124359131, -5.648200035095215, -5.021699905395508, -4.809299945831299, -4.821199893951416, -5.259399890899658, -5.105599880218506, -4.205999851226807, -5.291399955749512, -4.862500190734863, -4.013500213623047, -4.581299781799316, -3.726900100708008, -3.872299909591675, -4.607100009918213, -4.652599811553955, -3.997999906539917, -4.954599857330322, -4.613900184631348, -4.655399799346924, -4.351200103759766, -4.1666998863220215, -4.515999794006348, -4.294600009918213, -4.581399917602539, -4.839799880981445, -4.704500198364258, -4.597499847412109, -4.728700160980225, -4.020599842071533, -4.109000205993652, -4.292500019073486, -4.365600109100342, -4.456299781799316, -4.366499900817871, -4.543099880218506, -4.556399822235107, -4.638000011444092, -4.624100208282471, -4.696800231933594, -4.860300064086914, -4.860499858856201, -4.896599769592285, -4.934000015258789, -4.973299980163574, -4.93779993057251, -3.605600118637085, -4.520299911499023, -4.698599815368652, -4.107999801635742, -4.56689977645874, -4.480100154876709, -4.142899990081787, -4.35860013961792, -4.40500020980835, -4.220900058746338, -4.280799865722656, -3.882499933242798, -4.125100135803223, -4.282299995422363, -4.334099769592285, -4.262899875640869, -3.4012999534606934, -4.084700107574463, -4.388700008392334, -4.472400188446045, -4.577400207519531, -4.694799900054932, -4.710700035095215, -4.82859992980957, -4.864299774169922, -4.920899868011475, -4.530600070953369, -4.86329984664917, -4.207699775695801, -4.593900203704834, -4.538899898529053, -4.6757001876831055, -4.983799934387207, -4.610899925231934, -5.031700134277344, -5.093800067901611, -4.878499984741211, -4.625999927520752, -4.603099822998047, -4.510799884796143, -4.370699882507324, -4.425300121307373, -4.462500095367432, -4.51170015335083, -4.451900005340576, -4.529399871826172, -4.041600227355957, -4.264200210571289, -4.350900173187256, -4.362400054931641, -4.484000205993652, -4.484399795532227, -4.537199974060059, -4.564899921417236, -4.6082000732421875, -4.732699871063232, -4.801199913024902, -4.894599914550781, -4.914400100708008, -4.74399995803833, -4.508900165557861, -4.141499996185303, -4.4720001220703125, -4.962500095367432, -5.191199779510498, -3.6707000732421875, -4.452899932861328, -4.126299858093262, -4.376699924468994, -4.122300148010254, -4.3856000900268555, -4.118899822235107, -4.191800117492676, -4.28380012512207, -4.392600059509277, -4.513000011444092]}, \"token.table\": {\"Topic\": [2, 2, 4, 1, 4, 2, 1, 4, 1, 2, 3, 3, 4, 4, 4, 1, 2, 3, 4, 3, 1, 3, 4, 1, 1, 1, 2, 2, 1, 2, 1, 3, 3, 4, 1, 3, 4, 2, 3, 2, 3, 3, 4, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 1, 2, 4, 2, 1, 1, 2, 2, 3, 2, 2, 4, 4, 2, 1, 1, 2, 3, 4, 2, 1, 2, 3, 4, 2, 1, 2, 3, 4, 4, 4, 2, 2, 2, 1, 2, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 3, 4, 2, 2, 1, 2, 3, 4, 1, 3, 4, 1, 3, 3, 4, 3, 3, 2, 3, 4, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 4, 3, 1, 2, 3, 4, 4, 1, 4, 4, 4, 1, 2, 3, 4, 2, 3, 1, 3, 4, 1, 4, 1, 2, 3, 2, 4, 1, 3, 4, 1, 3, 4, 1, 3, 4, 1, 3, 4, 3, 1, 2, 3, 3, 3, 3, 4, 4, 2, 3, 2, 4, 1, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 4, 2, 1, 2, 3, 3, 4, 1, 3, 4], \"Freq\": [0.9882751703262329, 0.969744861125946, 0.014919151552021503, 0.9766796231269836, 0.013198372907936573, 0.9946964979171753, 0.9183759689331055, 0.06957393884658813, 0.9793704748153687, 0.7322837114334106, 0.2609286904335022, 0.03249302878975868, 0.9585443735122681, 0.9878218770027161, 0.9962465167045593, 0.4032449722290039, 0.07467499375343323, 0.09956666082143784, 0.42315831780433655, 0.9789287447929382, 0.8619218468666077, 0.10985278338193893, 0.025350643321871758, 0.978681206703186, 0.9845842719078064, 0.012526675127446651, 0.9770806431770325, 0.9790031313896179, 0.11800407618284225, 0.8732301592826843, 0.22841280698776245, 0.7555192708969116, 0.9475808143615723, 0.04987267404794693, 0.6908772587776184, 0.16748538613319397, 0.1395711600780487, 0.337411493062973, 0.6654504537582397, 0.15514372289180756, 0.827433168888092, 0.23647204041481018, 0.748828113079071, 0.9795612692832947, 0.5119945406913757, 0.3688904643058777, 0.02862081304192543, 0.09222261607646942, 0.6752607822418213, 0.10768277943134308, 0.029164087027311325, 0.18844486773014069, 0.07067268341779709, 0.9275789260864258, 0.16586484014987946, 0.6450299620628357, 0.1904374063014984, 0.9886158108711243, 0.9779691696166992, 0.9772270321846008, 0.992825448513031, 0.9928246140480042, 0.9778896570205688, 0.9790043234825134, 0.9860438108444214, 0.9909096360206604, 0.9971787333488464, 0.9777040481567383, 0.9896525740623474, 0.3648373484611511, 0.20563560724258423, 0.066334068775177, 0.3648373484611511, 0.9914270639419556, 0.11246046423912048, 0.6549168229103088, 0.0992298275232315, 0.13230642676353455, 0.9907655119895935, 0.06412893533706665, 0.07214505970478058, 0.6172410249710083, 0.24048352241516113, 0.9961174726486206, 0.9929689168930054, 0.9943556189537048, 0.986439049243927, 0.9843311905860901, 0.06342273950576782, 0.9354854226112366, 0.8200336694717407, 0.03381581977009773, 0.14371724426746368, 0.5284817218780518, 0.057694509625434875, 0.1753913015127182, 0.23770137131214142, 0.42123574018478394, 0.3642450273036957, 0.0446014329791069, 0.16601644456386566, 0.23054631054401398, 0.030071256682276726, 0.6916389465332031, 0.050118762999773026, 0.9846725463867188, 0.9939539432525635, 0.8464258313179016, 0.15592055022716522, 0.413030743598938, 0.2150314599275589, 0.1383865922689438, 0.23419268429279327, 0.30039796233177185, 0.18570056557655334, 0.10377383977174759, 0.40417179465293884, 0.34988588094711304, 0.4428243339061737, 0.2022777795791626, 0.09500756114721298, 0.7283913493156433, 0.015834594145417213, 0.16626323759555817, 0.21164937317371368, 0.016280721873044968, 0.7814745903015137, 0.19779562950134277, 0.16369293630123138, 0.09548754245042801, 0.538822591304779, 0.844961404800415, 0.1536293476819992, 0.9762499928474426, 0.989291250705719, 0.5344083309173584, 0.07097610831260681, 0.22962859272956848, 0.16282755136489868, 0.854185938835144, 0.025371858850121498, 0.11840201169252396, 0.8446049690246582, 0.14293314516544342, 0.9082825183868408, 0.07675626873970032, 0.9833400249481201, 0.9969345331192017, 0.01388488244265318, 0.7497836351394653, 0.23604300618171692, 0.7662205100059509, 0.21654058992862701, 0.3433758616447449, 0.4303644001483917, 0.1739770919084549, 0.050361789762973785, 0.20609170198440552, 0.17174309492111206, 0.4367181360721588, 0.18646392226219177, 0.8759890198707581, 0.1194530501961708, 0.9969687461853027, 0.7980028986930847, 0.0070619722828269005, 0.12358451634645462, 0.07061972469091415, 0.9898983836174011, 0.05039985850453377, 0.9449973106384277, 0.9882012009620667, 0.9935762286186218, 0.6354555487632751, 0.22427843511104584, 0.021359849721193314, 0.11747917532920837, 0.701639711856842, 0.29542726278305054, 0.6439169645309448, 0.06231454387307167, 0.2908012270927429, 0.8672451972961426, 0.12691393494606018, 0.9860142469406128, 0.08421045541763306, 0.9022548794746399, 0.16989749670028687, 0.8282502889633179, 0.7448399662971497, 0.11321566998958588, 0.1430092751979828, 0.8243213295936584, 0.004431834910064936, 0.16840973496437073, 0.8851507902145386, 0.047208044677972794, 0.05901005491614342, 0.19333387911319733, 0.7960807085037231, 0.9946513175964355, 0.9816789031028748, 0.9881080985069275, 0.8277721405029297, 0.1635105460882187, 0.9968892335891724, 0.9814462065696716, 0.15595845878124237, 0.8421757221221924, 0.9910547733306885, 0.051617491990327835, 0.9291148781776428, 0.9743571281433105, 0.02008983865380287, 0.7771535515785217, 0.04857209697365761, 0.17000234127044678, 0.7298867106437683, 0.04293451085686684, 0.2208060622215271, 0.006133501883596182, 0.6356901526451111, 0.09535352885723114, 0.2689458429813385, 0.3806094825267792, 0.08760059624910355, 0.24769823253154755, 0.2839467525482178, 0.7896111011505127, 0.03589141368865967, 0.08972853422164917, 0.08075568079948425, 0.8204504251480103, 0.13023023307323456, 0.03906906768679619, 0.22694377601146698, 0.4012041687965393, 0.31204769015312195, 0.056735944002866745, 0.515764057636261, 0.2419070303440094, 0.05477140098810196, 0.18713562190532684, 0.5501442551612854, 0.12046954780817032, 0.12850084900856018, 0.20479823648929596, 0.9946516156196594, 0.9763102531433105, 0.9109574556350708, 0.07007364928722382, 0.017518412321805954, 0.26594939827919006, 0.7303054928779602, 0.12807144224643707, 0.43032002449035645, 0.43544289469718933], \"Term\": [\"address\", \"advance\", \"advance\", \"agree\", \"agree\", \"appreciated\", \"argument\", \"argument\", \"atheism\", \"available\", \"available\", \"average\", \"average\", \"ball\", \"baseball\", \"better\", \"better\", \"better\", \"better\", \"built\", \"case\", \"case\", \"case\", \"cause\", \"claim\", \"code\", \"code\", \"color\", \"computer\", \"computer\", \"control\", \"control\", \"cost\", \"cost\", \"course\", \"course\", \"course\", \"data\", \"data\", \"date\", \"date\", \"defense\", \"defense\", \"development\", \"does\", \"does\", \"does\", \"does\", \"don\", \"don\", \"don\", \"don\", \"earth\", \"earth\", \"edu\", \"edu\", \"edu\", \"email\", \"evidence\", \"exist\", \"file\", \"files\", \"flight\", \"format\", \"ftp\", \"game\", \"games\", \"gif\", \"god\", \"good\", \"good\", \"good\", \"good\", \"graphics\", \"help\", \"help\", \"help\", \"help\", \"hi\", \"high\", \"high\", \"high\", \"high\", \"hit\", \"home\", \"image\", \"images\", \"info\", \"information\", \"information\", \"isn\", \"isn\", \"isn\", \"just\", \"just\", \"just\", \"just\", \"know\", \"know\", \"know\", \"know\", \"large\", \"large\", \"large\", \"large\", \"launch\", \"league\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"ll\", \"ll\", \"ll\", \"ll\", \"long\", \"long\", \"long\", \"looking\", \"looking\", \"looking\", \"looking\", \"lost\", \"lost\", \"lost\", \"lot\", \"lot\", \"lot\", \"lot\", \"low\", \"low\", \"mac\", \"mail\", \"make\", \"make\", \"make\", \"make\", \"mean\", \"mean\", \"mean\", \"means\", \"means\", \"money\", \"money\", \"moon\", \"nasa\", \"national\", \"national\", \"national\", \"near\", \"near\", \"need\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"ones\", \"ones\", \"orbit\", \"people\", \"people\", \"people\", \"people\", \"pitching\", \"play\", \"play\", \"player\", \"players\", \"point\", \"point\", \"point\", \"point\", \"program\", \"program\", \"really\", \"really\", \"really\", \"reason\", \"reason\", \"religion\", \"research\", \"research\", \"runs\", \"runs\", \"said\", \"said\", \"said\", \"say\", \"say\", \"say\", \"saying\", \"saying\", \"saying\", \"science\", \"science\", \"season\", \"shuttle\", \"simply\", \"software\", \"software\", \"space\", \"station\", \"team\", \"team\", \"teams\", \"technology\", \"technology\", \"thanks\", \"thanks\", \"thing\", \"thing\", \"thing\", \"things\", \"things\", \"things\", \"things\", \"think\", \"think\", \"think\", \"time\", \"time\", \"time\", \"time\", \"true\", \"true\", \"true\", \"true\", \"understand\", \"understand\", \"understand\", \"use\", \"use\", \"use\", \"use\", \"ve\", \"ve\", \"ve\", \"ve\", \"way\", \"way\", \"way\", \"way\", \"win\", \"windows\", \"words\", \"words\", \"words\", \"year\", \"year\", \"years\", \"years\", \"years\"]}, \"R\": 20, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 4, 3, 1]};\n\nfunction LDAvis_load_lib(url, callback){\n  var s = document.createElement('script');\n  s.src = url;\n  s.async = true;\n  s.onreadystatechange = s.onload = callback;\n  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n  document.getElementsByTagName(\"head\")[0].appendChild(s);\n}\n\nif(typeof(LDAvis) !== \"undefined\"){\n   // already loaded: just create the visualization\n   !function(LDAvis){\n       new LDAvis(\"#\" + \"ldavis_el141400058176215048693157103\", ldavis_el141400058176215048693157103_data);\n   }(LDAvis);\n}else if(typeof define === \"function\" && define.amd){\n   // require.js is available: use it to load d3/LDAvis\n   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n   require([\"d3\"], function(d3){\n      window.d3 = d3;\n      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n        new LDAvis(\"#\" + \"ldavis_el141400058176215048693157103\", ldavis_el141400058176215048693157103_data);\n      });\n    });\n}else{\n    // require.js not available: dynamically load d3 & LDAvis\n    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n                 new LDAvis(\"#\" + \"ldavis_el141400058176215048693157103\", ldavis_el141400058176215048693157103_data);\n            })\n         });\n}\n</script>"},"metadata":{}}]},{"cell_type":"code","source":" #topic -> frequency of words\n    \n    ldaModel.show_topics(formatted=False)","metadata":{"_uuid":"6e54346202aeae964221c175721e490f82ac093f","execution":{"iopub.status.busy":"2022-11-01T10:12:42.269015Z","iopub.execute_input":"2022-11-01T10:12:42.269633Z","iopub.status.idle":"2022-11-01T10:12:42.286544Z","shell.execute_reply.started":"2022-11-01T10:12:42.269551Z","shell.execute_reply":"2022-11-01T10:12:42.285339Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"[(0,\n  [('year', 0.025458157),\n   ('game', 0.017570145),\n   ('like', 0.016263098),\n   ('think', 0.01620684),\n   ('good', 0.016142676),\n   ('team', 0.015898217),\n   ('just', 0.015119392),\n   ('games', 0.014063816),\n   ('time', 0.01379057),\n   ('baseball', 0.012894804)]),\n (1,\n  [('don', 0.024067786),\n   ('think', 0.020810897),\n   ('just', 0.018352589),\n   ('people', 0.018069614),\n   ('like', 0.015503514),\n   ('say', 0.014905944),\n   ('know', 0.013641485),\n   ('does', 0.012891971),\n   ('way', 0.010932128),\n   ('thing', 0.010241804)]),\n (2,\n  [('space', 0.033328358),\n   ('nasa', 0.016827945),\n   ('earth', 0.014880039),\n   ('new', 0.0126420595),\n   ('orbit', 0.012416884),\n   ('years', 0.011970996),\n   ('time', 0.01165617),\n   ('long', 0.01153383),\n   ('launch', 0.011420396),\n   ('high', 0.010989247)]),\n (3,\n  [('thanks', 0.027171412),\n   ('know', 0.020599812),\n   ('graphics', 0.01794252),\n   ('information', 0.016441157),\n   ('mail', 0.016423808),\n   ('does', 0.01616163),\n   ('program', 0.015877381),\n   ('edu', 0.014685668),\n   ('like', 0.014081165),\n   ('help', 0.01383218)])]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Tasks\n### <br> 1. Annotate the topics. I.e., label them based on the word distrubution.\n\n\n","metadata":{"_uuid":"d1c2a6341c459514aea6004eda612e5a0eb8fab1"}},{"cell_type":"code","source":"categories=['alt.atheism','comp.graphics','rec.sport.baseball','sci.space']\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:12:42.288445Z","iopub.execute_input":"2022-11-01T10:12:42.288843Z","iopub.status.idle":"2022-11-01T10:12:42.293978Z","shell.execute_reply.started":"2022-11-01T10:12:42.288772Z","shell.execute_reply":"2022-11-01T10:12:42.292889Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"trainDataset.target[0:10]","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:12:42.295743Z","iopub.execute_input":"2022-11-01T10:12:42.296209Z","iopub.status.idle":"2022-11-01T10:12:42.307898Z","shell.execute_reply.started":"2022-11-01T10:12:42.296022Z","shell.execute_reply":"2022-11-01T10:12:42.306642Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"array([1, 1, 0, 3, 3, 3, 1, 0, 1, 1])"},"metadata":{}}]},{"cell_type":"code","source":"trainDataset.data[7]","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:12:42.309548Z","iopub.execute_input":"2022-11-01T10:12:42.310051Z","iopub.status.idle":"2022-11-01T10:12:42.319962Z","shell.execute_reply.started":"2022-11-01T10:12:42.309987Z","shell.execute_reply":"2022-11-01T10:12:42.318907Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"\" \\n(Deletion)\\n(Deletion)\\n \\nAn universe it has created. By the way, can you tell me why it is less\\ntyrannic to let one of one's own creatures do what it likes to others?\\nBy your definitions, your god has created Satan with full knowledge what\\nwould happen - including every choice of Satan.\\n \\nCan you explain us what Free Will is, and how it goes along with omniscience?\\nDidn't your god know everything that would happen even before it created the\\nworld? Why is it concerned about being a tyrant when noone would care if\\neverything was fine for them? That the whole idea comes from the possibility\\nto abuse power, something your god introduced according to your description?\\n \""},"metadata":{}}]},{"cell_type":"code","source":"#Q1\n\n#  categories = ['sci.space','alt.atheism','comp.graphics','rec.sport.baseball']\n\n#make annotaions to the topics - give appropiate label or name to the topic - we know the topic we started with\n#we have 4 category so we know the topics we need to match them to these results\n#we have different order now, we need to match it to the new indices\n\n\n#we should write comment with topic nums and name mapping\ntopicAnnotation = ['rec.sport.baseball','alt.atheism','sci.space','comp.graphics'] \ntopicAnnotation","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:12:42.321861Z","iopub.execute_input":"2022-11-01T10:12:42.322182Z","iopub.status.idle":"2022-11-01T10:12:42.330877Z","shell.execute_reply.started":"2022-11-01T10:12:42.322134Z","shell.execute_reply":"2022-11-01T10:12:42.330019Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"['rec.sport.baseball', 'alt.atheism', 'sci.space', 'comp.graphics']"},"metadata":{}}]},{"cell_type":"markdown","source":"### <br> 2. Build an accuracy function and explain it and how the characteristics of the LDA are used in the propused accuracy model.","metadata":{}},{"cell_type":"code","source":"#Q2\n\n#for each doc i need to match the topic, use lda, each topic is freq of words, for each doc we need the freq,\n#we build evaluation function, show the train and test words in topics, we can use lda 0.5 one topic and 0.5 other topic.\n#allow overlapping topics\n\n#model uses the traindateset and there is predict function \n# model gets docs and then for each doc check the probability\n# later we need to choose topic \n# than we check succsess (presents)\n\n#  categories = ['alt.atheism','comp.graphics','rec.sport.baseball','sci.space']\n#  topicAnnotation = ['rec.sport.baseball','alt.atheism','sci.space','comp.graphics'] \n\n\ndef accuracy(ldaModel, corpusTopicModeling, topicAnnotation, trainDataset, trainTarget):\n    topicsMap = {0:1, 3:2, 1:3 ,2:0}\n    hitRate = 0\n    for i in range(len(trainTarget)):\n        topic = topicsMap[trainTarget[i]]\n        doc_freq = ldaModel[corpusTopicModeling[i]]\n        values = []\n        UpTo40 = False\n        topicProb = 0\n        for j in range(len(doc_freq)):\n            values.append(doc_freq[j][1])\n            if doc_freq[j][0]==topic: \n                if doc_freq[j][1] >= 0.4:\n                    hitRate+=1\n                    UpTo40=True\n                else:\n                    topicProb = doc_freq[j][1]\n                break\n        if not UpTo40:\n            maxNum=max(values)\n            if topicProb == maxNum:\n                    hitRate+=1  \n                    \n            \n    return hitRate/len(trainTarget)\n    \n\naccuracy(ldaModel, corpusTopicModeling, topicAnnotation, trainDataset, trainTarget)\n#accuracy\n","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:12:42.332501Z","iopub.execute_input":"2022-11-01T10:12:42.332809Z","iopub.status.idle":"2022-11-01T10:12:43.071901Z","shell.execute_reply.started":"2022-11-01T10:12:42.332735Z","shell.execute_reply":"2022-11-01T10:12:43.070892Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0.823409669211196"},"metadata":{}}]},{"cell_type":"markdown","source":"### <br> 3. Evalute the model on the train dataset.\n","metadata":{}},{"cell_type":"code","source":"#Q3\n\n#run the function on train\n\naccuracy(ldaModel, corpusTopicModeling, topicAnnotation, trainDataset, trainTarget)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:12:43.073464Z","iopub.execute_input":"2022-11-01T10:12:43.073814Z","iopub.status.idle":"2022-11-01T10:12:43.813899Z","shell.execute_reply.started":"2022-11-01T10:12:43.07373Z","shell.execute_reply":"2022-11-01T10:12:43.812521Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0.8229007633587786"},"metadata":{}}]},{"cell_type":"markdown","source":"### <br> 4. Evalute the model on the test dataset.\n","metadata":{}},{"cell_type":"code","source":"#Q4\n#build dictionary by test\n#run the function on test\n\n\n\ndata2 = cv.fit_transform(testDocs)\ndata2 = pd.DataFrame(data2.toarray(), columns=cv.get_feature_names())\ncols2 = data2.columns\nbt2 = data2.apply(lambda x: x > 0)\nbt2 = bt2.apply(lambda x: list(cols2[x.values]), axis=1) #instead of numbers we want to see the words\ndictionary2 = Dictionary(bt2)\n# convert tokenized documents into a document-term matrix\ncorpusTopicModeling2 = [dictionary2.doc2bow(text) for text in bt2]\nprint(accuracy(ldaModel, corpusTopicModeling2, topicAnnotation, testDataset, testTarget))\n","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:12:43.818268Z","iopub.execute_input":"2022-11-01T10:12:43.818608Z","iopub.status.idle":"2022-11-01T10:12:45.640082Z","shell.execute_reply.started":"2022-11-01T10:12:43.818551Z","shell.execute_reply":"2022-11-01T10:12:45.638887Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"0.5362318840579711\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### <br> 5. Explain the difference in the evaluations.\n","metadata":{}},{"cell_type":"code","source":"#Q5\n\n#do we get diff numbers? why? write in comments\n#otherwise same numbers? why?\n\n#As you can see the train data set accuracy function returned value is ~0.82 and the test data set accuracy function retured ~0.53.\n#The reason for this differance is that the LDA model trained by the train set and it get overfitted.\n#That's why the model doesn't handle the test set as well as the train set.\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:12:45.642189Z","iopub.execute_input":"2022-11-01T10:12:45.642499Z","iopub.status.idle":"2022-11-01T10:12:45.648294Z","shell.execute_reply.started":"2022-11-01T10:12:45.642447Z","shell.execute_reply":"2022-11-01T10:12:45.647057Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}